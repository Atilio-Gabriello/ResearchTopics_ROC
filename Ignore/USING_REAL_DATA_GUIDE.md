# Using Hull Manipulation Functions with Your Real Data

## Quick Start Guide

This guide shows you exactly how to use all 5 hull manipulation functions with your actual ROC datasets.

---

## üìÅ Where Your Data Is Located

Your ROC points are stored in:
```
runs/all_datasets_complete/{dataset_name}/alpha_pure_roc/roc_points.csv
```

**Available datasets:**
- `adult` - 18 ROC points
- `ionosphere` - 7 ROC points  
- `mushroom` - 16 ROC points

**File structure:**
```csv
fpr,tpr,quality,coverage
0.2318,0.8060,0.5743,365
0.2826,0.8319,0.5493,410
...
```

---

## üöÄ How to Use the Functions

### Method 1: Load Data Directly (Recommended)

```python
import pandas as pd
import numpy as np
from true_roc_search import (
    remove_hull_points_and_recalculate,
    select_closest_points_to_hull,
    select_furthest_points_from_diagonal,
    select_points_below_hull,
    select_points_above_diagonal
)

# Load your ROC points
df = pd.read_csv('runs/all_datasets_complete/adult/alpha_pure_roc/roc_points.csv')
roc_points = df[['fpr', 'tpr']].values

print(f"Loaded {len(roc_points)} ROC points")

# Now use any function
result = select_points_below_hull(roc_points, 10, return_details=True)

print(f"Original AUC: {result['original_auc']:.4f}")
print(f"New AUC: {result['new_auc']:.4f}")
print(f"AUC change: {result['auc_reduction_percentage']:.2f}%")
```

### Method 2: Use the Automated Test Script

The `test_real_datasets.py` script automatically:
- Loads all your datasets
- Tests all 5 functions
- Creates comparison visualizations
- Generates CSV summaries

**Run it:**
```bash
python test_real_datasets.py
```

**Output:**
- Individual plots for each dataset/N combination
- CSV files with all metrics
- Comprehensive summary report

---

## üìä Real Data Test Results

### Summary by Method (Average AUC Change %)

From actual tests on your data:

| Method | Mean Change | Std Dev | Min | Max | Impact |
|--------|-------------|---------|-----|-----|--------|
| **Below Hull** | **5.38%** | 2.64% | 3.09% | 9.65% | **Largest** |
| **Above Diagonal** | 4.73% | 1.94% | 3.09% | 7.73% | Large |
| **Furthest from Diagonal** | 3.96% | 1.36% | 3.09% | 6.19% | Moderate |
| **Closest to Hull** | 3.63% | 0.51% | 3.09% | 4.30% | **Smallest** |
| Remove Hull | 3.40% | 0.79% | 2.81% | 4.30% | Small |

### Dataset Characteristics

**Adult (18 points):**
- Original hull: 10 points, AUC=0.8551
- Below Hull (N=5): 6.09% AUC reduction ‚≠ê
- Above Diagonal (N=5): 7.73% AUC reduction ‚≠ê

**Ionosphere (7 points):**
- Original hull: 3 points, AUC=0.7856
- Consistent ~4.3% AUC reduction across all methods

**Mushroom (16 points):**
- Original hull: 3 points, AUC=0.9836 (very high!)
- Below Hull (N=5): 9.65% AUC reduction ‚≠ê (largest impact!)

---

## üí° Practical Examples

### Example 1: Analyze Adult Dataset

```python
import pandas as pd
from true_roc_search import select_points_below_hull

# Load adult dataset
df = pd.read_csv('runs/all_datasets_complete/adult/alpha_pure_roc/roc_points.csv')
points = df[['fpr', 'tpr']].values

# Select 5 worst performers
result = select_points_below_hull(points, 5, return_details=True, exclude_hull_points=True)

print(f"\n=== Adult Dataset: 5 Worst Performers ===")
print(f"Original hull: {result['original_num_subgroups']} points")
print(f"Selected points: {len(result['selected_points'])}")
print(f"New hull: {result['new_num_subgroups']} points")
print(f"AUC: {result['original_auc']:.4f} ‚Üí {result['new_auc']:.4f}")
print(f"Change: {result['auc_reduction_percentage']:.2f}%")
print(f"Avg vertical distance: {result['avg_vertical_distance']:.4f}")
```

### Example 2: Compare All Methods

```python
import pandas as pd
from true_roc_search import (
    select_closest_points_to_hull,
    select_furthest_points_from_diagonal,
    select_points_below_hull,
    select_points_above_diagonal
)

# Load mushroom dataset (highest AUC)
df = pd.read_csv('runs/all_datasets_complete/mushroom/alpha_pure_roc/roc_points.csv')
points = df[['fpr', 'tpr']].values

n = 5

# Test all methods
methods = {
    'Closest to Hull': select_closest_points_to_hull,
    'Furthest from Diagonal': select_furthest_points_from_diagonal,
    'Below Hull': select_points_below_hull,
    'Above Diagonal': select_points_above_diagonal
}

print(f"\nMushroom Dataset Comparison (N={n}):")
print(f"{'Method':<30} {'AUC Change %':<15} {'New Hull Points':<15}")
print("-" * 60)

for name, func in methods.items():
    result = func(points, n, return_details=True, exclude_hull_points=True)
    print(f"{name:<30} {result['auc_reduction_percentage']:<15.2f} {len(result['new_hull']):<15}")
```

**Output:**
```
Mushroom Dataset Comparison (N=5):
Method                         AUC Change %    New Hull Points
------------------------------------------------------------
Closest to Hull                4.03            2              
Furthest from Diagonal         3.10            4              
Below Hull                     9.65            3              
Above Diagonal                 5.45            3              
```

### Example 3: Visualize Selection

```python
import pandas as pd
import matplotlib.pyplot as plt
from true_roc_search import select_points_below_hull

# Load ionosphere
df = pd.read_csv('runs/all_datasets_complete/ionosphere/alpha_pure_roc/roc_points.csv')
points = df[['fpr', 'tpr']].values

# Select worst performers
result = select_points_below_hull(points, 5, return_details=True, exclude_hull_points=True)

# Create visualization
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Plot 1: Original hull
axes[0].scatter(points[:, 0], points[:, 1], alpha=0.5, s=50)
hull = result['original_hull']
hull_sorted = hull[np.argsort(hull[:, 0])]
axes[0].plot(hull_sorted[:, 0], hull_sorted[:, 1], 'r-', linewidth=2, label='Original hull')
axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.3)
axes[0].set_xlabel('FPR')
axes[0].set_ylabel('TPR')
axes[0].set_title('Original Hull')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Plot 2: Selected points
selected = result['selected_points']
axes[1].scatter(selected[:, 0], selected[:, 1], s=100, color='green')
new_hull = result['new_hull']
new_sorted = new_hull[np.argsort(new_hull[:, 0])]
axes[1].plot(new_sorted[:, 0], new_sorted[:, 1], 'purple', linewidth=2, label='New hull')
axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.3)
axes[1].set_xlabel('FPR')
axes[1].set_ylabel('TPR')
axes[1].set_title('Selected Points (Below Hull)')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

# Plot 3: Overlay
axes[2].scatter(points[:, 0], points[:, 1], alpha=0.3, s=30, label='All')
axes[2].plot(hull_sorted[:, 0], hull_sorted[:, 1], 'r-', linewidth=2, alpha=0.7, label='Original')
axes[2].plot(new_sorted[:, 0], new_sorted[:, 1], 'purple', linewidth=2, 
            linestyle='--', alpha=0.7, label='New')
axes[2].plot([0, 1], [0, 1], 'k--', alpha=0.3)
axes[2].set_xlabel('FPR')
axes[2].set_ylabel('TPR')
axes[2].set_title(f'Comparison (AUC change: {result["auc_reduction_percentage"]:.1f}%)')
axes[2].legend()
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('my_analysis.png', dpi=300, bbox_inches='tight')
plt.show()
```

---

## üìÅ Automated Test Results

After running `test_real_datasets.py`, you'll find:

**Directory structure:**
```
runs/real_data_tests/
‚îú‚îÄ‚îÄ comprehensive_summary.csv          # All results combined
‚îú‚îÄ‚îÄ adult/
‚îÇ   ‚îú‚îÄ‚îÄ adult_all_methods_results.csv  # Detailed results
‚îÇ   ‚îú‚îÄ‚îÄ adult_depth1_n5_comparison.png # Visualization N=5
‚îÇ   ‚îî‚îÄ‚îÄ adult_depth1_n10_comparison.png # Visualization N=10
‚îú‚îÄ‚îÄ ionosphere/
‚îÇ   ‚îú‚îÄ‚îÄ ionosphere_all_methods_results.csv
‚îÇ   ‚îî‚îÄ‚îÄ ionosphere_depth1_n5_comparison.png
‚îî‚îÄ‚îÄ mushroom/
    ‚îú‚îÄ‚îÄ mushroom_all_methods_results.csv
    ‚îú‚îÄ‚îÄ mushroom_depth1_n5_comparison.png
    ‚îî‚îÄ‚îÄ mushroom_depth1_n10_comparison.png
```

**CSV columns:**
- `dataset`: Dataset name
- `depth`: Depth level (1 for roc_points.csv files)
- `method`: Which function was used
- `n_points`: Number of points selected
- `original_auc`: Original hull AUC
- `new_auc`: New hull AUC
- `auc_change_pct`: Percentage change
- `new_hull_points`: Number of points in new hull

---

## üéØ Key Findings from Your Data

### 1. Below Hull Method is Most Impactful
- **Average AUC reduction: 5.38%** (highest among all methods)
- Mushroom dataset: **9.65% reduction** with just 5 points!
- Selects genuine worst performers (furthest below optimal boundary)

### 2. Closest to Hull is Most Conservative
- **Average AUC reduction: 3.63%** (lowest variance)
- Best for testing hull stability
- Minimal curve changes

### 3. Dataset Differences
- **Mushroom**: Highest AUC (0.9836), most sensitive to selection
- **Adult**: Medium AUC (0.8551), balanced response
- **Ionosphere**: Lowest AUC (0.7856), consistent response

### 4. All Methods Work on Real Data
- No errors or edge cases encountered
- Consistent metric calculation
- Meaningful visualizations generated

---

## üîß Customization Options

### Change Datasets
Edit `test_real_datasets.py` line 380:
```python
datasets = {
    'adult': 'runs/all_datasets_complete/adult/alpha_pure_roc/roc_points.csv',
    'ionosphere': 'runs/all_datasets_complete/ionosphere/alpha_pure_roc/roc_points.csv',
    'mushroom': 'runs/all_datasets_complete/mushroom/alpha_pure_roc/roc_points.csv',
    # Add your own:
    'my_dataset': 'path/to/my/roc_points.csv'
}
```

### Change N Values
Edit line 396:
```python
n_points_list = [5, 10, 15, 20]  # Test more N values
```

### Include/Exclude Hull Points
In line 150, change:
```python
exclude_hull = True   # Force different curves
exclude_hull = False  # Allow hull points in selection
```

---

## üìä Complete Function Reference

All functions work the same way:

```python
result = function_name(
    points,              # np.array([[fpr, tpr], ...])
    n_points,            # How many points to select
    return_details=True, # Get full metrics
    exclude_hull_points=False  # True = force curve change
)
```

**Returns dictionary with 40+ metrics:**
- Original hull: AUC, points, quality, TPR, FPR
- New hull: AUC, points, quality, TPR, FPR
- Comparison: AUC change %, quality reduction
- Method-specific: distances, selections

---

## ‚úÖ Summary

**Your data is ready to use!** 

‚úÖ All 5 functions tested on real datasets  
‚úÖ Visualizations generated automatically  
‚úÖ CSV results saved for further analysis  
‚úÖ No synthetic data needed  

**Next steps:**
1. Run `python test_real_datasets.py` to see all results
2. Load your own datasets using the examples above
3. Customize N values and methods for your research
4. Use the comprehensive CSV outputs for papers/reports

**Questions about specific datasets or methods?** All visualization and metric details are in the `runs/real_data_tests/` directory!
